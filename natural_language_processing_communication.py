# -*- coding: utf-8 -*-
"""Natural Language Processing - Communication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9uQEkhupDsaWszHADYLgfNmJggRpKrH

# Objectives

- Natural Language Processing (NLP) - Its important for most cases
- TextBlob, NLTK, textatistic and spaCy Library 
- Tokenize text into words and sentences
- Part of speech tagging
- Sentiment Analysis - Positive, Negative, Neutral
- Detecting Language of text and tranlate between languages
- Word roots -  stemming and lemmatization
- spell checking and correction
- Remove the stop words from the text 
- Readability assesment
- Named entity recognition and similarity detection

# Examples of Natural Language Communication

- Conversation between two people
- Learning a foreign language
- Using smartphone to read menu
- Reading / writting text messages
- Blind - braille or listening to the screen reader
- Email - Spanish or French - English

# Text collections - corpora or plural corpus

- Tweets
- Facebook posts
- Conversations
- Movie Review
- Documents
- Books
- News

- Nuances of the meaning - makes NL understanding - difficult
- Meaning can be influenced by context and reader's view of the world

# TextBlob: Simplified text processing

- Object oriented NLP ext processing - built on NLTK and pattern NLP Libraries
- Jobs:
    - Tokenization- spliting text into pieces of tokens - words or numbers
    - Parts-to-speech (POS) tagging - noun, verb, adjective, 
    - Noun Phrase Extraction - `red brick factory`
        - Is a red brick factory a factory that makes red bricks?
        - Is it a red factory that makes bricks of any colour?
        - Is it a factory built of red bricks that makes products of any type?
        - Music group - pop
    - Sentiment Analysis
    - Inter- Language translations or detecting the language - google translate
    - Inflection - pluralizing or singularizing words
    - Spell checking and spelling correction
    - Stemming - varieties - varieti
    - Lemmatization - varieties - variety- generate real wirds based on word's context
    - WordNet Integration
        - Word Frequncies
        - Stop word elimination - as, a, an, the, I, we
        - n-grams - producing sers of consecutive words in a corpus for use inn indentifying words that frequently appears adjacent to each other

# Project Gutenberg
- Rich source of text for analysis - free e-books

# Create a TextBlob
"""

from textblob import TextBlob

text = 'Today is a beautiful day. Tomorrow looks like bad weather'

blob=TextBlob(text)

blob

# TextBl0b does supports various methods and comparisons - Senstences, words
import nltk
nltk.download('punkt')

blob.sentences

blob.words

blob.word_counts

"""# Part of Speech Tagging

- Evaluate words based on context - determine POS - determining the meaning
- Nouns, Pronouns, Verbs, Adjectives, Adverbs, Prepositions, Conjuctions and   Interjection - {HA!, Yes!}
- Sub-Categories -
     - Meaning of words - `Set` and `Run`
"""

blob

import nltk
nltk.download('averaged_perceptron_tagger')

blob.tags

"""- TextBlob - patternTagger 63 parts of speech tags
     - NN - Sindular or mass noun
     - VBZZ- third person singular present verb
     - DT- determiner- (The an that my this their)
     - JJ- Adjective
     - NNP-Proper Singular Noun
     - IN- Subbordinating conjunction or preposition

# Extracting Noun Phrases
"""

blob

import nltk
 nltk.download('brown')

blob.noun_phrases

"""# Sentiment Analysis with the TextBlob Default Sentiment Analyzer
- Positive, Negative, Neutral
- Food is not good, the movie was excellent, the movie was not bad
- Deep Learninng

"""

blob

blob.sentiment

"""- Polarity -> (-1 Neg),(1Pos), (0Neu)
- Subjectivity -> 0.0 - Objective , 1.0 - Subjective
"""

text2="I will be making a statement tonight. A big WIN!"
blob2=TextBlob(text2)

text3="My #AmericanDreamPlan is a promise to Hispanic Americans to fuel a thriving economy, provide education opportunity for all, preserve freedom, and support faith, family, and community! "
blob3=TextBlob(text3)

blob3

blob.sentiment

# Commented out IPython magic to ensure Python compatibility.
# %precision 3

blob.sentiment.polarity

blob.sentiment.subjectivity

blob3

for sentences in blob.sentences:
    print(sentences.sentiment)

"""# Sentiment Analysis with the NaiveBayesAnalyzer"""

from textblob.sentiments import NaiveBayesAnalyzer

text

blob=TextBlob(text, analyzer=NaiveBayesAnalyzer())

blob

import nltk
 nltk.download('movie_reviews')

blob.sentiment

for sentences in blob.sentences:
    print(sentences.sentiment)

""" # Language Detection and Translation

- Near Real-Time Translation
- IBM Watson - Inter-Language Translation
"""

blob

blob.detect_language()

spanish= blob.translate(to='es')

blob

spanish

spanish.detect_language()

chinese=blob.translate(to='zh')

chinese

swahili=blob.translate(to='sw')

swahili

chinese.translate()

swahili.translate()

spanish.translate()

"""# Infleation: Pluralization and Singularization
- Infletions are diffrent forms of the same words, such as singular and  plural, - people person and different verbs tenses - run ran
- Word Freq - wadnt to convert all the inflected wordds to the same form -- more accurate word frequencies
"""

from textblob import Word

index = Word('index')

index

index.pluralize()

cacti=Word('cacti')

cacti.singularize()

cactus=Word('cactus')

cactus.pluralize()

from textblob import TextBlob

animals=TextBlob('Dog Cat Fish Bird').words

animals.pluralize()

"""# Harvesting Data from Twitter
- Tweepy > 3.7
- geopy

"""

import tweepy #This is the Gateway to using Twitter APIs

"""- Authenticate with the Twitter"""

import keys

"""   - We need to create and configure an OAuthHandler to Authenticate Twitter """

auth=tweepy.OAuthHandler(keys.consumer_key,keys.consumer_secret)

auth.set_access_token(keys.access_token,keys.access_token_secret)

"""# We must take note of two methods:
- `wait_on_rate_limit=True` -- tweepy wait for 15 minutes each time gets to the limit -- violation
- `wait_on_rate_limit_notify=True` -- displaya command line message -- hit the rate timit 
"""

api=tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)

# Getting Information About Twitter Account

trump=api.get_user('realDonaldTrump')

nasa=api.get_user('nasa')

nasa.id

trump.id

trump.name

nasa.name

nasa.description

trump.description

trump.status.text

nasa.status.text

nasa.followers_count

trump.followers_count

obama=api.get_user('BarackObama')

obama.status.text

obama.followers_count

obama.friends_count

me=api.me()

me.description

me.name

"""#Excercise
-- Status
-- 4 accounts
-- sentiment analysis-inbuilt one and nayebayesclasifier

# Spell Checking

- Free of spelling errors.
- Word's spellchech method - list of tuples - correct spelling- confidence values
- Theyr for They
"""

from textblob import Word

word=Word('massanger')

word.spellcheck()

word.correct()

from textblob import TextBlob

sentence = TextBlob("This is an importnt session, since it loks at natural languag")

sentence.correct()

"""# Normalization: Stemming and Lemmatization
    - Stemming - Removing the prefix and the surfix -Not a real word meaning
    - Lemmatization - Same- result - Real World Meaning
    
    - Normalization - program, programs, Programming
"""

from textblob import Word

word = Word('Varieties')

word.stem()

import nltk
nltk.download('wordnet')

word.lemmatize()

"""# Word Frequencies

- Get Similarities between documents - word frequencies
- TextBlob can count the word frequencies for you
- Path's read_text() - reads closes
"""

from pathlib import Path

from textblob import TextBlob

blob = TextBlob(Path('Romeo and Juliet.txt').read_text())

blob.word_counts['juliet']

blob.words.count('Juliet')

blob.word_counts['romeo']

blob.words.count('Romeo')

blob.noun_phrases.count('lady capulet')

"""# Getting Definitions, Synonyms,and Antonyms from WordNet
 - Using the WordNet -English word Datasabe - Definitions sunonyms ans antonyms
"""

from textblob import Word

happy = Word('happy')

happy.definitions

happy.define()

happy.synsets

"""synset - Group of synonyms
- Word's lemmitized form
- a - Adjectives n- Nouns V-Verbs r- Adverbs s- Adjectivr satellite
"""

synonyms=set()

for synset in happy.synsets:
    for lemma in synset.lemmas():
        synonyms.add(lemma.name())

synonyms

lemmas = happy.synsets[0].lemmas()

lemmas

lemmas[0].antonyms()

#lemmas[2].antonyms()

"""# Stop Words
- Common Words taht are often removed before analysis -> useful iinformation
- nlth - stopwords
"""

import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords

stops = stopwords.words('english')

stopsFrench= stopwords.words('french')

stops

stopsFrench

from textblob import TextBlob

blob = TextBlob('Today is a beautiful day.')

[words for words in blob.words if words not in stops]

"""# N-Grams

 - A sequence of n text items - letters in words or words in sentences
 - Identify Letters or words that appear frequently 
     - Predictive text input
     - speech to text input 
     
"""

text = 'Today is a beautiful day. Tomorrow looks bad weather.'

blob =TextBlob(text)

#ngrams() n=4 = 6 default is 3

blob.ngrams(n=3)

"""# Visualizing Word Frequencies with Bar Chart and Word Clouds

* top 20 words - remove the stopwords
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

#Load the Data
from pathlib import Path

from textblob import TextBlob

blob = TextBlob(Path('Romeo and Juliet.txt').read_text())

from nltk.corpus import stopwords

#get the frequencies of the words

items=blob.word_counts.items()

stop_words=stopwords.words('english')

#elimiate stop words
items =[item for item in items if item[0] not in stop_words]

"""# Sorting the words Frequency
- Descrending order
- to specify the tuple element we use the itemgetter function - operator module
"""

from operator import itemgetter

sorted_items=sorted(items, key=itemgetter(1), reverse=True)

sorted_items[len(sorted_items)-1]

len(sorted_items)

top20=sorted_items[:20]

sorted_items[0]

#convert to dataframe

import pandas as pd

df=pd.DataFrame(top20, columns=['words','count'])

df

import matplotlib.pyplot as plt

axes=df.plot.bar(x='words',y='count', legend=False)

"""`Assignment_03` - project Guternburg - (2 books) analysis - bar plot of the top 30 words == cloud

# Visualizing Word Frequencies with Word Cloud

- Open source wordcloud module -> word clouds
- matplot lib for ploting

# Loading of the Text
"""

from pathlib import Path



text = Path('Romeo and Juliet.txt').read_text()

text

"""- We load the mask image that specifies the word cloud's shape
- The WordCloud fills non-white areas of a mask image with the text
- Load the mask filling using the imread function from imageio module found in Anaconda
"""

import imageio

mask_image=imageio.imread('mask_heart.png')

"""- Configuring the WordCloud Object
- 400 by 200 pixels - default image size
- Colors used are assigned randomly - from the color map

"""

from wordcloud import WordCloud

wordcloud =WordCloud(width=1000,height=1000,colormap='prism',mask=mask_image,background_color='white')

"""- WordCloud has a method called generate() 
- receives the text as an argument ad creates the word cloud
"""

wordcloud=wordcloud.generate(text)

"""- Removes the stop words, from the text argument using the inbuilt stop word list 
- Calculates word frequnces for the remaing words 
- Builds the cloud with maximum 200 by default -> max-words
"""

# Saving the Image

wordcloud=wordcloud.to_file('RomeoandJulietHeart.png')

"""# Generate Word Cloud from a dictionary 

- If you have a WordCloud's fit_words method to create a word cloud from the dictionary 
- Does not remove the stop words from dic
"""

import matplotlib.pyplot as plt

plt.imshow(wordcloud)

"""# Wordcloud on Twitter"""

import tweepy

import keys

auth = tweepy.OAuthHandler(keys.consumer_key, keys.consumer_secret)

auth.set_access_token(keys.access_token, keys.access_token_secret)

api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)

nasa=api.get_user('nasa')

nasa.status.text

nasa.followers_count

nasa.friends_count

"""# Tweepy `Cursors` : Getting An Account's Followers and Friends

- Twitter API methods will return collection of objects
    - Tweets in your Twitter Timeline
    - Another Accounts Timeline
- Timeline for tweets sent by a user ny the user's friends
- Max Items to returned - pages of results
- JSON - mare pages to get - Cursor handles the paging items
- Invokes a method and checks if there is another page of results
- API object is configures to wait on rate limits.

#  Determining and Account Followers

- Followers method call Twitter's followers/ list method - 20 by default upto 200
- Grab 10 followers of NASA Account
"""

#Create a cursor
followers=[]

cursor=tweepy.Cursor(api.followers,screen_name='nasa')

cursor

for account in cursor.items(10):
    followers.append(account.screen_name)

print('Followers:',' ' .join(sorted(followers, key=lambda s: s.lower())))

"""### Automatic Paging

To get up to 200 followers at a time, create the Cursor with the count keyword argument
"""

cursor=tweepy.Cursor(api.followers,screen_name='nasa', count=200)

"""### Determining Whom an account follows

- API object friends method
- calls the twitter friendlist method
- user objects
- 20 by default - 200 at a time
- 15 times even 15 minutes
"""

friends=[]

cursor=tweepy.Cursor(api.friends,screen_name='nasa')

for friend in cursor.items(10):
  friends.append(friend.screen_name)

print('Friends: ', ' '.join(sorted(friends, key=lambda s: s.lower())))

"""### Getting a User's Recent tweets

- API object user_timeline -tweets from an account and account;s friends
-staus/user_timeline method
- 20 tweets as Status Object -200
- 3200 most recent tweets
- 1500 times every 15 minutes
"""

nasa_tweets=api.user_timeline(screen_name='nasa', count=3)

for tweet in nasa_tweets:
  print(f'{tweet.user.screen_name}: {tweet.text}\n')

api.home_timeline()

"""### Searching Recent Tweeets

- Via the API method search
- previous 7 days tweets
- not guaranteed returns
-15 tweets by default at a time=100
"""

from textblob import TextBlob

def print_tweets(tweets):
    """For each Tweepy Status object in tweets, display the user's screen_name and tweet text
    If the language is not English, translate the text with TextBlob."""
    for tweet in tweets:
        print(f'{tweet.user.screen_name}:', end=' ')
        if 'en' in tweet.lang:
            print(f'{tweet.text}\n')
        elif 'und' not in tweet.lang: 
            print(f'\n ORIGINAL: {tweet.text}')
            print(f'TRANSLATED: {TextBlob(tweet.text).translate()}\n')

tweets=api.search(q='Mars Rover',count=6)

print_tweets(tweets)



tweets=api.search(q='from: nasa since:2020-13-11',count=15)

print_tweets(tweets)

"""### Searching for a hashtag

- hasgtag # indicates something important like a trending topic
"""

tweets=api.search(q='#uhuru', count=3)

print_tweets(tweets)

"""### Spotting Trends: Twitter Trends API

- Going viral
- maintaining a list of trending topics worldwide
- Twitter Trends API  can return a list of trending topic locations and the top 50 trending topics

places with trnding topics
"""

trends_available=api.trends_available()

len(trends_available)

trends_available[32]

world_trends=api.trends_place(id=1)

world_trends

trends_list= world_trends[0] ['trends']

trends_list[2]